---
title: "R Notebook"
output: html_notebook
---

```{r}
# packages used
library(tidyverse)
```


```{r}
# read data
imdb <- read.csv("imdb_top_1000.csv")
head(imdb)

# remove commas from Gross variable
imdb <- imdb %>%
  mutate(Gross = gsub(pattern = "[,]", replacement = "", Gross))

# make Gross numeric and remove NAs
imdb$Gross <- as.numeric(imdb$Gross)
imdb <- na.omit(imdb)
```

## Question 1

a.

```{r}
# fit IMDB_Rating and Gross to linear model
gross_imdb <- lm(IMDB_Rating ~ Gross, data = imdb)
summary(gross_imdb)
```

```{r}
# fit IMDB_Rating and Meta_score to linear model
meta_imdb <- lm(IMDB_Rating ~ Meta_score, data = imdb)
summary(meta_imdb)
```

```{r}
# predict IMDB Rating from gross (model 1)
Y1 <- predict(gross_imdb)
E1 <- mean((imdb$IMDB_Rating - Y1)^2)

# predict IMDB Rating from meta score (model 2)
Y2 <- predict(meta_imdb)
E2 <- mean((imdb$IMDB_Rating - Y2)^2)
c(E1,E2)
```

```{r}
# leave-one-out cross validation

#Initialize
N <- nrow(imdb)
predictors <- imdb[c("Gross","Meta_score")]
E_m1 <- numeric(N)
E_m2 <- numeric(N)
# CV Loop
for (i in 1:N)
{
  # Fit the models without the i^th data point
  m1_i <- lm(IMDB_Rating ~ Gross, data = imdb[-i,])
  m2_i <- lm(IMDB_Rating ~ Meta_score, data = imdb[-i,])
  # Predict CHD for the unobserved data point i
  Y1 <- predict(m1_i, newdata = predictors[i,])
  Y2 <- predict(m2_i, newdata = predictors[i,])
  # Compute the error
  E_m1[i] <- (imdb$IMDB_Rating[i] - Y1)^2
  E_m2[i] <- (imdb$IMDB_Rating[i] - Y2)^2
}

```

```{r}
c(mean(E_m1),mean(E_m2))

```

b.

```{r}
# fit Meta_score and Gross to linear model
gross_meta <- lm(Meta_score ~ Gross, data = imdb)
summary(gross_meta)
```

```{r}
# fit Meta_score and IMDB_Rating to linear model
imdb_meta <- lm(Meta_score ~ IMDB_Rating, data = imdb)
summary(imdb_meta)
```

```{r}
# predict meta score from gross (model 1)
Y1 <- predict(gross_meta)
E1 <- mean((imdb$Meta_score - Y1)^2)

# predict meta score from imdb rating (model 2)
Y2 <- predict(imdb_meta)
E2 <- mean((imdb$Meta_score - Y2)^2)
c(E1,E2)
```

```{r}
# leave-one-out cross validation

#Initialize
N <- nrow(imdb)
predictors <- imdb[c("Gross","IMDB_Rating")]
E_m1 <- numeric(N)
E_m2 <- numeric(N)
# CV Loop
for (i in 1:N)
{
  # Fit the models without the i^th data point
  m1_i <- lm(Meta_score ~ Gross, data = imdb[-i,])
  m2_i <- lm(Meta_score ~ IMDB_Rating, data = imdb[-i,])
  # Predict CHD for the unobserved data point i
  Y1 <- predict(m1_i, newdata = predictors[i,])
  Y2 <- predict(m2_i, newdata = predictors[i,])
  # Compute the error
  E_m1[i] <- (imdb$Meta_score[i] - Y1)^2
  E_m2[i] <- (imdb$Meta_score[i] - Y2)^2
}

```

```{r}
c(mean(E_m1),mean(E_m2))

```

## Question 2

a.

```{r}
K <- 10000

beta_hat = meta_imdb$coefficients

e_hat = meta_imdb$residuals

bhat_boot <- matrix(nrow=K, ncol = length(beta_hat))

for (k in 1:K){
  xboot = sample(imdb$Meta_score, size = 100, replace = T)
  eboot = sample(e_hat, size = 100, replace = T)
  yboot = beta_hat[1] + beta_hat[2]*xboot + eboot
  mboot = lm(yboot ~ xboot)
  bhat_boot[k,] = mboot$coefficients
}

```

```{r}
par(mfrow = c(1,2))
hist(bhat_boot[,1])
abline(v=beta_hat[1], lwd = 3, col = 'red')
hist(bhat_boot[,2])
abline(v=beta_hat[2], lwd = 3, col = 'red')

```

```{r}
alpha = 0.05
CI_int = quantile(bhat_boot[,1], probs = c(alpha/2, 1-alpha/2))
CI_slp = quantile(bhat_boot[,2], probs = c(alpha/2, 1-alpha/2))

```

```{r}
par(mfrow = c(1,2))
hist(bhat_boot[,1])
abline(v=beta_hat[1], lwd=3, col='red')
abline(v=CI_int, lwd=3, col='blue')
hist(bhat_boot[,2])
abline(v=beta_hat[2], lwd=3, col='red')
abline(v=CI_slp, lwd=3, col='blue')

```

b.

```{r}
K <- 10000

beta_hat = imdb_meta$coefficients

e_hat = imdb_meta$residuals

bhat_boot <- matrix(nrow=K, ncol = length(beta_hat))

for (k in 1:K){
  xboot = sample(imdb$IMDB_Rating, size = 100, replace = T)
  eboot = sample(e_hat, size = 100, replace = T)
  yboot = beta_hat[1] + beta_hat[2]*xboot + eboot
  mboot = lm(yboot ~ xboot)
  bhat_boot[k,] = mboot$coefficients
}

```

```{r}
par(mfrow = c(1,2))
hist(bhat_boot[,1])
abline(v=beta_hat[1], lwd = 3, col = 'red')
hist(bhat_boot[,2])
abline(v=beta_hat[2], lwd = 3, col = 'red')

```

```{r}
alpha = 0.05
CI_int = quantile(bhat_boot[,1], probs = c(alpha/2, 1-alpha/2))
CI_slp = quantile(bhat_boot[,2], probs = c(alpha/2, 1-alpha/2))

```

```{r}
par(mfrow = c(1,2))
hist(bhat_boot[,1])
abline(v=beta_hat[1], lwd=3, col='red')
abline(v=CI_int, lwd=3, col='blue')
hist(bhat_boot[,2])
abline(v=beta_hat[2], lwd=3, col='red')
abline(v=CI_slp, lwd=3, col='blue')

```

## Question 3

```{r}
# Observed Test Statistic
Tobs <- abs(cor(imdb$IMDB_Rating, imdb$Meta_score))
#Monte Carlo
K <- 10000
# number of permutations used
Tperm <- numeric(K)
for (i in 1:K)
  {
  imdb_i <- sample(imdb$IMDB_Rating)
  meta_i <- sample(imdb$Meta_score)
  Tperm[i] <- abs(cor(imdb_i, meta_i))
  }

hist(Tperm, breaks=50)
abline(v=Tobs, lwd=3, col = 'red')

```

```{r}
mean(Tperm > Tobs)

```

## EDA

```{r}
hist(imdb$IMDB_Rating)
range(imdb$IMDB_Rating)

```

```{r}
hist(imdb$Meta_score)
range(imdb$Meta_score)

```



